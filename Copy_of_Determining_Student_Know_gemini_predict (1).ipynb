{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U3y0JUEVu_Zu"
      },
      "outputs": [],
      "source": [
        "!pip install google-auth google-auth-httplib2 google-auth-oauthlib google-api-python-client\n",
        "!pip install -q -U google-generativeai\n",
        "!pip install --upgrade google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA5yFjUmrkRm"
      },
      "outputs": [],
      "source": [
        "#Import necessary libraries\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IkheaXGsir1"
      },
      "outputs": [],
      "source": [
        "import openpyxl\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import RequestOptions\n",
        "from google.api_core import retry\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbiAI6m8sL3J"
      },
      "outputs": [],
      "source": [
        "# Import the API Key (insert key here)\n",
        "GOOGLE_API_KEY=''\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OntCAzYIuhN0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmGkH0WztTIc"
      },
      "source": [
        "# Reading G-Drive Sheet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vy8VfjjdtJsh"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet gspread oauth2client google-auth\n",
        "import gspread\n",
        "from google.colab import auth\n",
        "import pandas as pd\n",
        "import google.auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Authorize gspread client\n",
        "# Use google.auth.default() to get credentials\n",
        "credentials, project = google.auth.default()\n",
        "gc = gspread.authorize(credentials)\n",
        "\n",
        "def load_sheet(sheet_id, sheet_name=\"Sheet1\"):\n",
        "    # Load google sheet as dataframe\n",
        "    worksheet = gc.open_by_key(sheet_id).worksheet(sheet_name)\n",
        "    data = worksheet.get_all_values()\n",
        "    df = pd.DataFrame(data[1:], columns=data[0])\n",
        "    return df\n",
        "\n",
        "path = \"1qOK_HrWcAYrG-VllwoaoR8HoBuKPQSTu2b30wrg2mS4\"\n",
        "sheet = load_sheet(path,\"Determining What Students Know Predict_responses\")  # There is only one sheet, so no ambiguity here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uKQIYXct0_j"
      },
      "source": [
        "Get all the responses and scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaaEXbQ_tjSs"
      },
      "outputs": [],
      "source": [
        "INPUT_COLUMN = \"response\" # Specify the column from which input is read (A->1, B->2, etc.)\n",
        "row_count = len(sheet)\n",
        "inputs = [sheet.loc[i, INPUT_COLUMN] for i in range(row_count)]\n",
        "inputs = [x for x in inputs if x is not None]\n",
        "print(inputs)\n",
        "print(len(inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qASEYdU1vH08"
      },
      "source": [
        "# Gemini Prompt Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKg2bQ7qvi_i"
      },
      "source": [
        "Scoring Prompt Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfyoLemuvkrN"
      },
      "outputs": [],
      "source": [
        "GEMINI_SYSTEM_PROMPT = \"\"\"\n",
        "You are a tutor evaluator. Please score the following tutor response to a tutor training scenario involving a middle school student as follows:\n",
        "-If the tutor’s response asks the student to explain what the student already knows or assess their prior knowledge, or offers support as the student attempts the problem themselves, score it with a 1. Examples of responses scoring a 1 are: “What information can you get from the figure above?”; “Looks like you've started solving the problem. Can you share a little bit more about how you came up with the first part of your answer?”; “What ideas do you have on how to start this problem?”; “That looks great so far.  Can you explain what is the meaning of the letters?”; “This is a solid start. It looks like you're using 'PEMDAS,' the acronym for the 'order of operations' in math, and 'P' stands for 'parentheses.' Since you already identified and simplified the parenthese well in that first step, what do you think the next step, ‘E,’ stands for? Is that something you have in your notes from class?”\n",
        "-If the tutor's response directly tells the student what to do or asks a specific content-related question, particularly if it is a yes or no question, score it with a 0. Sample responses scoring a 0 include: “Hi Roberto, we will figure this problem together , okay?”; “After parenthesis and then exponents, perform the indicated multiplication or division, follow the order, from left to right.  So for example, looking at your problem, perform the 5/5 first before the multiplication by 2. See how we did the division first then multiplication?”; “Sure no problem. So 2 sides of the triangle are of equal length, correct?”; “On scale from 1-4 (1-easy, 4- difficult ), how difficult is that question ?”; “Sure no problem. So 2 sides of the triangle are of equal length, correct?  (An isosceles triangle.)”\n",
        "Once given a response by the user, please return a JSON string following the format, {\"Rationale\": \"your reasoning here\", \"Score\":0/1}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuVI8e8kxT87"
      },
      "source": [
        "Helper function for response parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIlxCT-ixV5J"
      },
      "outputs": [],
      "source": [
        "def extract_response(response_obj, json=False):\n",
        "  role = response_obj.choices[0].message.role\n",
        "  content = response_obj.choices[0].message.content\n",
        "  if json:\n",
        "    return {\"role\": role, \"content\": content}\n",
        "  else:\n",
        "    return (role, content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIHBTOqTwW1T"
      },
      "source": [
        "## gemini API Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms4Iojj3wWGw"
      },
      "outputs": [],
      "source": [
        "# Iterate over all responses\n",
        "MAX_TOKENS = 300\n",
        "TEMPERATURE = 0\n",
        "RUN_UP_TO = 256  # Sets a maximum index for responses to run. Useful to specify how many responses we want to run on (partial execution). Set to -1 to run them all.\n",
        "TWO_STAGE_INCLUDE_RESPONSE = False # Specifies whether the second-stage prompt uses the original response.\n",
        "SCORE_COLUMN = \"response\"  # Change column numbers here to  modify where output is written\n",
        "\n",
        "\n",
        "MODEL = 'gemini-2.5-pro'\n",
        "model = genai.GenerativeModel(MODEL, system_instruction=GEMINI_SYSTEM_PROMPT)\n",
        "\n",
        "if RUN_UP_TO >=  0:  # If an upper bound is set\n",
        "  inputs_upto = inputs[:RUN_UP_TO]\n",
        "else:\n",
        "  inputs_upto = inputs  # Take the whole set of responses\n",
        "tempScoreList = []\n",
        "tempDirectionList = []\n",
        "tempRationaleList = []\n",
        "retries = []\n",
        "\n",
        "for index, inpt in tqdm(enumerate(inputs_upto), total=len(inputs_upto)):\n",
        "  if index%60 == 0:\n",
        "    time.sleep(30)\n",
        "  generation_prompt = \"Tutor Response: \" + inpt + \"\\n\\n. Your JSON: \" # Gemini Change\n",
        "  generation_config = genai.GenerationConfig(temperature=TEMPERATURE)\n",
        "  # Use client.chat.completions.create instead of client.generate_text\n",
        "  gemini_out = model.generate_content(generation_prompt, generation_config=generation_config,request_options=RequestOptions(retry=retry.Retry(initial=1.0,multiplier=2.0,maximum=60.0))) # TODO: Potential break point\n",
        "  # Extract the content from the response\n",
        "  content = gemini_out.text.lstrip(\"```json\")[:-4]\n",
        "  # We now need to parse the JSON into rational and score\n",
        "  try:\n",
        "    content_json = json.loads(content)  # Run response through JSON\n",
        "    score = str(content_json[\"Score\"])  # Cast to string to avoid type inequality\n",
        "    rationale = str(content_json[\"Rationale\"])  # Fetch the rationale\n",
        "    sheet.at[index,\"GPT Score\"] = score  # Now write both into the dataframe\n",
        "    sheet.at[index,\"GPT Rationale\"] = rationale\n",
        "    tempScoreList.append(score)\n",
        "    tempDirectionList.append(generation_prompt)\n",
        "    tempRationaleList.append(rationale)\n",
        "    #rationale_cell.value = rationale\n",
        "\n",
        "  except:\n",
        "    print(\"error!\")\n",
        "    tempScoreList.append(\"error during LLM evaluation\")\n",
        "    tempRationaleList.append(\"error during LLM evaluation\")\n",
        "    retries.append((index,inpt))\n",
        "  #   score_cell.value = \"---\"\n",
        "  #   rationale_cell.value = \"---\"  # Failsafe\n",
        "  #   if TWO_STAGE:\n",
        "  #     refined_feedback_cell.value = \"---\"\n",
        "print(tempScoreList)\n",
        "print(tempDirectionList)\n",
        "print(tempRationaleList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXIjAG544f3I"
      },
      "outputs": [],
      "source": [
        "print(tempScoreList)\n",
        "print(tempRationaleList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUPYEvCt4bP9"
      },
      "outputs": [],
      "source": [
        "for i in range(len(inputs_upto)):\n",
        "  print(inputs_upto[i])\n",
        "  print(tempScoreList[i])\n",
        "  print(tempRationaleList[i])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzJmfvbvUaiU"
      },
      "outputs": [],
      "source": [
        "print(list(sheet[\"GPT Score\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7MWojh7Y0pg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvd9C9skROrl"
      },
      "source": [
        "## Save G-Drive Sheet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GckhEy2oRNme"
      },
      "outputs": [],
      "source": [
        "sheet.to_csv('/content/drive/Shareddrives/PLUS/Research/Clara Stuff/Determining_What_Students_Know_Gemini_Predict_Eval.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}