{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U3y0JUEVu_Zu"
      },
      "outputs": [],
      "source": [
        "!pip install google-auth google-auth-httplib2 google-auth-oauthlib google-api-python-client\n",
        "!pip install -q -U google-generativeai\n",
        "!pip install --upgrade google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EA5yFjUmrkRm"
      },
      "outputs": [],
      "source": [
        "#Import necessary libraries\n",
        "from google.oauth2 import service_account\n",
        "from googleapiclient.discovery import build"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IkheaXGsir1"
      },
      "outputs": [],
      "source": [
        "import openpyxl\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import google.generativeai as genai\n",
        "from google.generativeai.types import RequestOptions\n",
        "from google.api_core import retry\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbiAI6m8sL3J"
      },
      "outputs": [],
      "source": [
        "# Import the API Key (insert key here)\n",
        "GOOGLE_API_KEY=''\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmGkH0WztTIc"
      },
      "source": [
        "# Reading G-Drive Sheet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vy8VfjjdtJsh"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet gspread oauth2client google-auth\n",
        "import gspread\n",
        "from google.colab import auth\n",
        "import pandas as pd\n",
        "import google.auth\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Authorize gspread client\n",
        "# Use google.auth.default() to get credentials\n",
        "credentials, project = google.auth.default()\n",
        "gc = gspread.authorize(credentials)\n",
        "\n",
        "def load_sheet(sheet_id, sheet_name=\"Sheet1\"):\n",
        "    # Load google sheet as dataframe\n",
        "    worksheet = gc.open_by_key(sheet_id).worksheet(sheet_name)\n",
        "    data = worksheet.get_all_values()\n",
        "    df = pd.DataFrame(data[1:], columns=data[0])\n",
        "    return df\n",
        "\n",
        "path = \"1NHZZ5YklIMuMicMwZghzKfmLGBTVDI6lzRVIJyqVrU4\"\n",
        "sheet = load_sheet(path,\"Determining What Students Know Explain_responses\")  # There is only one sheet, so no ambiguity here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uKQIYXct0_j"
      },
      "source": [
        "Get all the responses and scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaaEXbQ_tjSs"
      },
      "outputs": [],
      "source": [
        "INPUT_COLUMN = \"response\" # Specify the column from which input is read (A->1, B->2, etc.)\n",
        "row_count = len(sheet) # Get number of rows. If number of rows to read is already known (or sheet does not terminate where data terminates), place number here (+1) instead.inputs = [sheet.loc[i, INPUT_COLUMN]  for i in range(2, row_count)]  # Column number is set here. YOUR INPUT COLUMN NUMBER GOES HERE.\n",
        "inputs = [sheet.loc[i, INPUT_COLUMN] for i in range(row_count)]\n",
        "inputs = [x for x in inputs if x is not None]\n",
        "print(inputs)\n",
        "print(len(inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qASEYdU1vH08"
      },
      "source": [
        "# Gemini Prompt Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKg2bQ7qvi_i"
      },
      "source": [
        "Scoring Prompt Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wfyoLemuvkrN"
      },
      "outputs": [],
      "source": [
        "GEMINI_SYSTEM_PROMPT = \"\"\"\n",
        "You are a tutor evaluator. Please score the following tutor response to a tutor training scenario involving a middle school student as follows:\n",
        "-If the tutor's response demonstrates that the tutor understands the importance of students explaining what they already know or the importance of asking students open-ended questions to assess their prior knowledge, score it with a 1. Sample responses scoring a 1 include: “because it would help me assess his prior knowledge about the order of operations”; “It acknowledges the work done and elicits an answer on student's understanding of next step”; “Introducing terminology he isn't familiar with might be confusing, so I would only use the term 'order of operations' if he introduced it. I also wouldn't give him the next step, I would see if he could figure out what it could be. Asking what he could do next gives a sense of what he knows and I can ask follow up questions to gain more insight”; “Asking them what they think should happen next allows them to tell you what they understand about the question coming into it”; “It engages him and let’s him build on things that he already knows to solve problems.”\n",
        "-If the tutor's response does not demonstrate that the tutor understands the importance of asking open questions and assessing student’s prior knowledge or what they already know, score it with a 0. Sample responses scoring a 0 include: “Roberto is struggling, but has some basic knowledge of the subject. He needs to be encouraged so that his self confidence can improve. He will strive to solve the problem”; “He looks like he is on the correct path”; “provides context for solving the problem, positive feedback, and encourages further work”; “It will encourage Roberto to keep on trying”; “Because it's helpful to acknowledge what is correct so far so the student can move forward.”\n",
        "Once given a response by the user, please return a JSON string following the format, {\"Rationale\": \"your reasoning here\", \"Score\":0/1}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WRT5urkcPKF"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuVI8e8kxT87"
      },
      "source": [
        "Helper function for response parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uIlxCT-ixV5J"
      },
      "outputs": [],
      "source": [
        "def extract_response(response_obj, json=False):\n",
        "  role = response_obj.choices[0].message.role\n",
        "  content = response_obj.choices[0].message.content\n",
        "  if json:\n",
        "    return {\"role\": role, \"content\": content}\n",
        "  else:\n",
        "    return (role, content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIHBTOqTwW1T"
      },
      "source": [
        "## gemini API Call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms4Iojj3wWGw"
      },
      "outputs": [],
      "source": [
        "# Iterate over all responses\n",
        "MAX_TOKENS = 300\n",
        "TEMPERATURE = 0\n",
        "RUN_UP_TO = 256  # Sets a maximum index for responses to run. Useful to specify how many responses we want to run on (partial execution). Set to -1 to run them all.\n",
        "TWO_STAGE = False  # Specifies whether to refine the feedback provided by the scoring prompt\n",
        "TWO_STAGE_INCLUDE_RESPONSE = False # Specifies whether the second-stage prompt uses the original response.\n",
        "SCORE_COLUMN = \"response\"  # Change column numbers here to  modify where output is written\n",
        "\n",
        "\n",
        "MODEL = 'gemini-2.5-pro'\n",
        "model = genai.GenerativeModel(MODEL, system_instruction=GEMINI_SYSTEM_PROMPT)\n",
        "\n",
        "if RUN_UP_TO >=  0:  # If an upper bound is set\n",
        "  inputs_upto = inputs[:RUN_UP_TO]\n",
        "else:\n",
        "  inputs_upto = inputs  # Take the whole set of responses\n",
        "tempScoreList = []\n",
        "tempDirectionList = []\n",
        "tempRationaleList = []\n",
        "retries = []\n",
        "\n",
        "for index, inpt in tqdm(enumerate(inputs_upto), total=len(inputs_upto)):\n",
        "  if index%60 == 0:\n",
        "    time.sleep(30)\n",
        "  generation_prompt = \"Tutor Response: \" + inpt + \"\\n\\n. Your JSON: \" # Gemini Change\n",
        "  generation_config = genai.GenerationConfig(temperature=TEMPERATURE)\n",
        "  # Use client.chat.completions.create instead of client.generate_text\n",
        "  gemini_out = model.generate_content(generation_prompt, generation_config=generation_config,request_options=RequestOptions(retry=retry.Retry(initial=1.0,multiplier=2.0,maximum=60.0))) # TODO: Potential break point\n",
        "  # Extract the content from the response\n",
        "  content = gemini_out.text.lstrip(\"```json\")[:-4]\n",
        "  try:\n",
        "    content_json = json.loads(content)  # Run response through JSON\n",
        "    score = str(content_json[\"Score\"])  # Cast to string to avoid type inequality\n",
        "    rationale = str(content_json[\"Rationale\"])  # Fetch the rationale\n",
        "    sheet.at[index,\"GPT Score\"] = score  # Now write both into the dataframe\n",
        "    sheet.at[index,\"GPT Rationale\"] = rationale\n",
        "    tempScoreList.append(score)\n",
        "    tempDirectionList.append(generation_prompt)\n",
        "    tempRationaleList.append(rationale)\n",
        "    #rationale_cell.value = rationale\n",
        "\n",
        "  except:\n",
        "    print(\"error!\")\n",
        "    tempScoreList.append(\"error during LLM evaluation\")\n",
        "    tempRationaleList.append(\"error during LLM evaluation\")\n",
        "    retries.append((index,inpt))\n",
        "  #   score_cell.value = \"---\"\n",
        "  #   rationale_cell.value = \"---\"  # Failsafe\n",
        "  #   if TWO_STAGE:\n",
        "  #     refined_feedback_cell.value = \"---\"\n",
        "print(tempScoreList)\n",
        "print(tempDirectionList)\n",
        "print(tempRationaleList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXIjAG544f3I"
      },
      "outputs": [],
      "source": [
        "print(tempScoreList)\n",
        "print(tempRationaleList)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cUPYEvCt4bP9"
      },
      "outputs": [],
      "source": [
        "for i in range(len(inputs_upto)):\n",
        "  print(inputs_upto[i])\n",
        "  print(tempScoreList[i])\n",
        "  print(tempRationaleList[i])\n",
        "  print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzJmfvbvUaiU"
      },
      "outputs": [],
      "source": [
        "print(list(sheet[\"GPT Score\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7MWojh7Y0pg"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pvd9C9skROrl"
      },
      "source": [
        "## Save G-Drive Sheet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GckhEy2oRNme"
      },
      "outputs": [],
      "source": [
        "sheet.to_csv('/content/drive/Shareddrives/PLUS/Research/Clara Stuff/Determining_What_Students_Know_Gemini_Explain_Eval.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}